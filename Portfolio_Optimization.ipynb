{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPiV1itMyxA3c/ZRsr2fiay",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ComputerFinance/ESWA/blob/main/Portfolio_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Libraries"
      ],
      "metadata": {
        "id": "tV2dZug9fy0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('/content')"
      ],
      "metadata": {
        "id": "8r0qpVE8k8Ci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r 'requirements.txt'"
      ],
      "metadata": {
        "id": "d8AZURcchQjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import scipy.cluster.hierarchy as sch\n",
        "import statsmodels.api as sm\n",
        "from tqdm import tqdm\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import cvxopt as opt\n",
        "from cvxopt import blas, solvers\n",
        "\n",
        "%matplotlib inline\n",
        "plt.close('all')\n",
        "\n",
        "plt.style.use(\"seaborn-whitegrid\")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "solvers.options['show_progress'] = False"
      ],
      "metadata": {
        "id": "9MEs4_hGYchx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Portfolio Optimization on MV, IVP and HRP"
      ],
      "metadata": {
        "id": "fn6QMJDyiZK3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the optimizations, we are based the implementations present in Prado (2016, 2018)."
      ],
      "metadata": {
        "id": "FRseJFGtifgg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXWeHbSjg6qA"
      },
      "outputs": [],
      "source": [
        "# On 20151227 by MLdP <lopezdeprado@lbl.gov>\n",
        "# Hierarchical Risk Parity\n",
        "\n",
        "\n",
        "def getIVP(cov, **kargs):\n",
        "\n",
        "    # Compute the inverse-variance portfolio\n",
        "    ivp = 1. / np.diag(cov)\n",
        "    ivp /= ivp.sum()\n",
        "    return ivp\n",
        "\n",
        "def getClusterVar(cov,cItems):\n",
        "\n",
        "    # Compute variance per cluster\n",
        "    cov_=cov.loc[cItems,cItems] # matrix slice\n",
        "    w_=getIVP(cov_).reshape(-1,1)\n",
        "    cVar=np.dot(np.dot(w_.T,cov_),w_)[0,0]\n",
        "    return cVar\n",
        "\n",
        "def getQuasiDiag(link):\n",
        "\n",
        "    # Sort clustered items by distance\n",
        "    link = link.astype(int)\n",
        "    sortIx = pd.Series([link[-1, 0], link[-1, 1]])\n",
        "    numItems = link[-1, 3]  # number of original items\n",
        "    while sortIx.max() >= numItems:\n",
        "        sortIx.index = range(0, sortIx.shape[0] * 2, 2)  # make space\n",
        "        df0 = sortIx[sortIx >= numItems]  # find clusters\n",
        "        i = df0.index\n",
        "        j = df0.values - numItems\n",
        "        sortIx[i] = link[j, 0]  # item 1\n",
        "        df0 = pd.Series(link[j, 1], index=i + 1)\n",
        "        sortIx = sortIx.append(df0)  # item 2\n",
        "        sortIx = sortIx.sort_index()  # re-sort\n",
        "        sortIx.index = range(sortIx.shape[0])  # re-index\n",
        "    return sortIx.tolist()\n",
        "\n",
        "def getRecBipart(cov, sortIx):\n",
        "\n",
        "    # Compute HRP alloc\n",
        "    w = pd.Series(1, index=sortIx)\n",
        "    cItems = [sortIx]  # initialize all items in one cluster\n",
        "    while len(cItems) > 0:\n",
        "        cItems = [i[j:k] for i in cItems for j, k in ((0, len(i) // 2), (len(i) // 2, len(i))) if len(i) > 1]  # bi-section\n",
        "        for i in range(0, len(cItems), 2):  # parse in pairs\n",
        "            cItems0 = cItems[i]  # cluster 1\n",
        "            cItems1 = cItems[i + 1]  # cluster 2\n",
        "            cVar0 = getClusterVar(cov, cItems0)\n",
        "            cVar1 = getClusterVar(cov, cItems1)\n",
        "            alpha = 1 - cVar0 / (cVar0 + cVar1)\n",
        "            w[cItems0] *= alpha  # weight 1\n",
        "            w[cItems1] *= 1 - alpha  # weight 2\n",
        "    return w\n",
        "\n",
        "def matrix_from_correlation(cov):\n",
        "  \n",
        "    vols = np.sqrt(np.diag(cov))\n",
        "    corr = np.multiply(cov,np.outer(vols**-1, vols**-1))\n",
        "    # deal with precision errors\n",
        "    np.fill_diagonal(corr.values, 1) #solve precision errors\n",
        "    distances = np.sqrt((1 - corr) / 2) \n",
        "    np.fill_diagonal(distances.values, 0) #solve precision errors\n",
        "    corr = pd.DataFrame(corr)\n",
        "    return corr\n",
        "\n",
        "def correlDist(corr):\n",
        "\n",
        "    # A distance matrix based on correlation, where 0<=d[i,j]<=1\n",
        "    # This is a proper distance metric\n",
        "    dist = ((1 - corr)/2.)**.5  # distance matrix\n",
        "    return dist\n",
        "\n",
        "\n",
        "def getHRP(cov):\n",
        "\n",
        "    corr = matrix_from_correlation(cov)\n",
        "    dist = correlDist(corr)\n",
        "    link = sch.linkage(dist, 'ward')\n",
        "    # dn = sch.dendrogram(link, labels=cov.index.values) #plot dendogram\n",
        "    sortIx = getQuasiDiag(link)\n",
        "    sortIx = corr.index[sortIx].tolist()\n",
        "    hrp = getRecBipart(cov, sortIx)\n",
        "    return hrp.sort_index()\n",
        "\n",
        "def getMV(cov):\n",
        "\n",
        "    cov = cov.T.values\n",
        "    n = len(cov)\n",
        "    N = 100\n",
        "    mus = [10 ** (5.0 * t / N - 1.0) for t in range(N)]\n",
        "\n",
        "    # Convert to cvxopt matrices\n",
        "    S = opt.matrix(cov)\n",
        "    pbar = opt.matrix(np.ones(cov.shape[0]))\n",
        "\n",
        "    # Create constraint matrices\n",
        "    G = -opt.matrix(np.eye(n))  # negative n x n identity matrix\n",
        "    h = opt.matrix(0.0, (n, 1))\n",
        "    A = opt.matrix(1.0, (1, n))\n",
        "    b = opt.matrix(1.0)\n",
        "\n",
        "    # Calculate efficient frontier weights using quadratic programming\n",
        "    portfolios = [solvers.qp(mu * S, -pbar, G, h, A, b)['x'] for mu in mus]\n",
        "    ## CALCULATE RISKS AND RETURNS FOR FRONTIER\n",
        "    returns = [blas.dot(pbar, x) for x in portfolios]\n",
        "    risks = [np.sqrt(blas.dot(x, S * x)) for x in portfolios]\n",
        "    ## CALCULATE THE 2ND DEGREE POLYNOMIAL OF THE FRONTIER CURVE\n",
        "    m1 = np.polyfit(returns, risks, 2)\n",
        "    x1 = np.sqrt(m1[2] / m1[0])\n",
        "    # CALCULATE THE OPTIMAL PORTFOLIO\n",
        "    wt = solvers.qp(opt.matrix(x1 * S), -pbar, G, h, A, b)['x']\n",
        "\n",
        "    return list(wt)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_portfolios(returns):\n",
        "    \n",
        "    cov = returns.cov()\n",
        "    hrp = getHRP(cov)\n",
        "    ivp = getIVP(cov)\n",
        "    ivp = pd.Series(ivp, index=cov.index)\n",
        "    mv = getMV(cov)\n",
        "    mv = pd.Series(mv, index=cov.index)\n",
        "    \n",
        "    portfolios = pd.DataFrame([mv, ivp, hrp], index=['MV', 'IVP', 'HRP']).T.sort_values(by='HRP',ascending=False)\n",
        "    \n",
        "    return portfolios,cov"
      ],
      "metadata": {
        "id": "Sr1c2asph1FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def historical_data():\n",
        "  \n",
        "  #Read dataset\n",
        "  prices = pd.read_excel('prices.xlsx').set_index('Date')\n",
        "\n",
        "  #Drop NaN\n",
        "  prices = prices.dropna()\n",
        "\n",
        "  #Normalize data\n",
        "  prices = prices/prices.iloc[0]\n",
        "\n",
        "  #Log returns\n",
        "  log_returns = (prices.pct_change(axis=0)).apply(np.log1p).iloc[1:,:]\n",
        "\n",
        "  #DataFrame Stocks\n",
        "  stocks_returns = log_returns.iloc[:, :-2]\n",
        "\n",
        "  #DataFrame Benchmarks\n",
        "  bench_returns = log_returns.iloc[:,-2:]\n",
        "\n",
        "  return stocks_returns,bench_returns"
      ],
      "metadata": {
        "id": "QC6lop1HW_Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Returned variables in functions\n",
        "\n",
        "stocks_returns = historical_data()[0]\n",
        "bench_returns = historical_data()[1]\n",
        "\n",
        "portfolios = get_all_portfolios(stocks_returns)[0]\n",
        "covar = get_all_portfolios(stocks_returns)[1]\n",
        "\n",
        "portfolios"
      ],
      "metadata": {
        "id": "NHGZHwGNViK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance Analysis"
      ],
      "metadata": {
        "id": "rRKSR_X-InYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def portfolios_performances(methods):\n",
        "\n",
        "  portfolio_order = portfolios.T.reindex(columns= stocks_returns.columns)\n",
        "\n",
        "  portfolio_methods = pd.DataFrame()\n",
        "\n",
        "  dict_performance = {'Cumulative Returns': [],\n",
        "                      'Expected Annual Return':[],\n",
        "                      'Annual Volatility':[],\n",
        "                      'Annual Variance':[],\n",
        "                      'Sharpe Ratio':[],\n",
        "                      'Max Drawdown':[],\n",
        "                      'Beta S&P500':[],\n",
        "                      'Beta Ibovespa':[]}\n",
        "\n",
        "  for i in methods:\n",
        "\n",
        "    weights = np.array(portfolio_order.loc[i])\n",
        "    invest = pd.DataFrame((stocks_returns * weights).sum(axis=1)).rename(columns={0:f'{i}'})\n",
        "\n",
        "    #Total Return\n",
        "    total_return = (((1+invest).cumprod())-1)*100\n",
        "    tr = total_return[i].values.tolist()[-1]\n",
        "    dict_performance['Cumulative Returns'].append(tr)\n",
        "\n",
        "    #Expected Annual Return\n",
        "    expected_return_annual = np.sum(invest.mean()*252)*100\n",
        "    dict_performance['Expected Annual Return'].append(expected_return_annual)\n",
        "\n",
        "    #Annual Volatility\n",
        "    invest_vol = np.sqrt(weights.T.dot(covar*252).dot(weights))*100\n",
        "    dict_performance['Annual Volatility'].append(invest_vol)\n",
        "\n",
        "    #Annual Variance\n",
        "    invest_var = np.sum(weights.T.dot(covar*252).dot(weights))*100\n",
        "    dict_performance['Annual Variance'].append(invest_var)\n",
        "\n",
        "    #Sharpe Ratio\n",
        "    sharpe = (expected_return_annual)/invest_vol\n",
        "    dict_performance['Sharpe Ratio'].append(sharpe)\n",
        "\n",
        "    #Max Drawdown\n",
        "    invest_cum = (1+invest).cumprod()\n",
        "    up = invest_cum.expanding(min_periods=1).max()\n",
        "    dd = (invest_cum/up)-1\n",
        "    drawdown = np.sum(dd.min())*100\n",
        "    dict_performance['Max Drawdown'].append(drawdown)\n",
        "\n",
        "    #Beta\n",
        "    Y = invest.values\n",
        "    X = bench_returns['Ibovespa'].values\n",
        "    Z = bench_returns['S&P500'].values\n",
        "\n",
        "    X = sm.add_constant(X)\n",
        "    Z = sm.add_constant(Z)\n",
        "\n",
        "    model1 = sm.OLS(Y,X)\n",
        "    result1 = model1.fit()\n",
        "    beta_invest_ibov = result1.params[1]\n",
        "    beta_invest_ibov = np.sum(beta_invest_ibov)\n",
        "    dict_performance['Beta Ibovespa'].append(beta_invest_ibov)\n",
        "\n",
        "    model2 = sm.OLS(Y,Z)\n",
        "    result2 = model2.fit()\n",
        "    beta_invest_sp500 = result2.params[1]\n",
        "    beta_invest_sp500 = np.sum(beta_invest_sp500)\n",
        "    dict_performance['Beta S&P500'].append(beta_invest_sp500)\n",
        "   \n",
        "    portfolio_temp = invest\n",
        "\n",
        "    if portfolio_methods.empty == True:\n",
        "      portfolio_methods = portfolio_temp\n",
        "\n",
        "    else:\n",
        "      portfolio_methods = pd.merge(portfolio_methods,portfolio_temp,left_index = True, right_index = True)\n",
        "      portfolio_methods = pd.DataFrame(portfolio_methods)\n",
        "  \n",
        "  #Performance DataFrame\n",
        "  df_performance = pd.DataFrame(data = dict_performance.values(), index = dict_performance.keys(), columns=['MV','IVP','HRP'])\n",
        "\n",
        "  #DataFrame Cumulative Returns Portfolios and Benchmarks\n",
        "  portfolios_methods_cum = (1+portfolio_methods).cumprod()\n",
        "  portfolios_methods_cum.iloc[0] = 1\n",
        "  portfolios_methods_cum = pd.DataFrame(portfolios_methods_cum)\n",
        "\n",
        "  bench_cum = (1+bench_returns).cumprod() #Cumulative returns Benchmarks\n",
        "  portfolios_methods_cum = pd.merge(portfolios_methods_cum,bench_cum,left_index = True, right_index = True)\n",
        "\n",
        "  ibov = portfolios_methods_cum.loc[portfolios_methods_cum.index[-1], \"Ibovespa\"]\n",
        "  ibov = (ibov-1)*100\n",
        "  sp500 = portfolios_methods_cum.loc[portfolios_methods_cum.index[-1], \"S&P500\"]\n",
        "  sp500 = (sp500-1)*100\n",
        "\n",
        "  print(f\"Cummulative Returns of the Ibovespa was \" + f\"{np.sum(ibov).round(2)}%\")\n",
        "  print(f\"Cummulative Returns of the S&P500 was \" + f\"{np.sum(sp500).round(2)}% \\n\")\n",
        "\n",
        "  plot = portfolios_methods_cum.plot(figsize = (20,10));\n",
        "\n",
        "  return df_performance, portfolios_methods_cum, plot"
      ],
      "metadata": {
        "id": "tL3q2czdTzWa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "performance, portfolios_cum, plot = portfolios_performances(['MV','IVP','HRP'])\n",
        "performance"
      ],
      "metadata": {
        "id": "hgRW1EREk0bJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monte Carlo Simulations of In-sample and Out-of-sample variances distributions"
      ],
      "metadata": {
        "id": "dn446_QHIsUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_cov_matrix():\n",
        "\n",
        "    return covar\n",
        "\n",
        "def compute_portfolio_variance(weights, cov):\n",
        "\n",
        "    return np.sum(np.array(weights).T.dot(cov*252).dot(np.array(weights)))\n",
        "\n",
        "def generate_returns_sample(covariances, horizon):\n",
        "  \n",
        "    return pd.DataFrame(np.random.multivariate_normal(np.zeros(len(covariances)), covariances,size=horizon))"
      ],
      "metadata": {
        "id": "XNovknNjkWlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "methods = {\n",
        "    'MV': getMV,\n",
        "    'IVP': getIVP,\n",
        "    'HRP': getHRP,\n",
        "}"
      ],
      "metadata": {
        "id": "oQ65BsWsm1a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def monte_carlo_experiment(simulations=10000):\n",
        "\n",
        "  empirical_variances = {method: {'in-sample' : [], 'out-sample': []} for method in methods.keys()}\n",
        "\n",
        "  experiments = simulations\n",
        "\n",
        "  for test in tqdm(range(experiments)):\n",
        "\n",
        "    true_covariances = sample_cov_matrix()\n",
        "    in_sample = generate_returns_sample(\n",
        "        true_covariances, horizon=260)\n",
        "    out_sample = generate_returns_sample(\n",
        "        true_covariances, horizon=260)\n",
        "    \n",
        "    for name, method in methods.items():\n",
        "\n",
        "        in_sample_weights = method(in_sample.cov())\n",
        "        \n",
        "        in_sample_var = compute_portfolio_variance(\n",
        "            in_sample_weights, in_sample.cov())\n",
        "    \n",
        "        out_sample_var = compute_portfolio_variance(\n",
        "            in_sample_weights, out_sample.cov())\n",
        "\n",
        "        empirical_variances[name][\n",
        "            'in-sample'].append(in_sample_var)\n",
        "        empirical_variances[name][\n",
        "            'out-sample'].append(out_sample_var)\n",
        "\n",
        "  for method, distribs in empirical_variances.items():\n",
        "    \n",
        "    plt.hist(distribs['in-sample'], bins=100, label='in-sample', alpha=0.7, color = 'darkgreen')\n",
        "    plt.hist(distribs['out-sample'], bins=100, label='out-sample', alpha=0.7,color = 'red')\n",
        "    plt.axvline(x=np.mean(distribs['in-sample']), color='darkgreen',linestyle='dashed', linewidth=3)\n",
        "    plt.axvline(x=np.mean(distribs['out-sample']), color='red',linestyle='dashed', linewidth=3)\n",
        "    plt.title(method,fontsize=15)\n",
        "    plt.legend(loc=1,fontsize=15)\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=15)\n",
        "    plt.grid(True, linewidth=0.3, linestyle='-')\n",
        "    sns.despine(left=True, bottom=True)\n",
        "    plt.plot(figsize = (30,24))\n",
        "    plt.show()\n",
        "    print('\\n')\n",
        "\n",
        "    print(method + ':\\n' +\n",
        "          'in-sample variance: ' + str(round(np.mean(distribs['in-sample']), 4)) + '\\n' +\n",
        "          'out-sample variance: ' + str(round(np.mean(distribs['out-sample']), 4)) + '\\n' +\n",
        "          '\\n\\n')"
      ],
      "metadata": {
        "id": "vc-KAXKekEOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mmc_in_out = monte_carlo_experiment(simulations=10000)\n",
        "mmc_in_out"
      ],
      "metadata": {
        "id": "XD2zfvIzlO0P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}